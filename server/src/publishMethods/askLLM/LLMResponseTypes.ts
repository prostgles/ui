import type { AnyObject } from "prostgles-types";

/**
 * Usage statistics for the completion request.
 */
type CompletionUsage = {
  /**
   * Only shown in anthropic responses.
   */
  cost?: number;

  /**
   * Number of tokens in the generated completion.
   */
  completion_tokens: number;

  /**
   * Number of tokens in the prompt.
   */
  prompt_tokens: number;

  /**
   * Total number of tokens used in the request (prompt + completion).
   */
  total_tokens: number;

  /**
   * Breakdown of tokens used in a completion.
   */
  completion_tokens_details?: CompletionTokensDetails;

  /**
   * Breakdown of tokens used in the prompt.
   */
  prompt_tokens_details?: PromptTokensDetails;
};

/**
 * Breakdown of tokens used in a completion.
 */
type CompletionTokensDetails = {
  /**
   * When using Predicted Outputs, the number of tokens in the prediction that
   * appeared in the completion.
   */
  accepted_prediction_tokens?: number;

  /**
   * Audio input tokens generated by the model.
   */
  audio_tokens?: number;

  /**
   * Tokens generated by the model for reasoning.
   */
  reasoning_tokens?: number;

  /**
   * When using Predicted Outputs, the number of tokens in the prediction that did
   * not appear in the completion. However, like reasoning tokens, these tokens are
   * still counted in the total completion tokens for purposes of billing, output,
   * and context window limits.
   */
  rejected_prediction_tokens?: number;
};

/**
 * Breakdown of tokens used in the prompt.
 */
type PromptTokensDetails = {
  /**
   * Audio input tokens present in the prompt.
   */
  audio_tokens?: number;

  /**
   * Cached tokens present in the prompt.
   */
  cached_tokens?: number;
};

/**
 * https://openrouter.ai/docs/api-reference/api-guides/overview
 */
type ErrorResponse = {
  code: number; // See "Error Handling" section
  message: string;
  metadata?: Record<string, unknown>; // Contains additional error information such as provider details, the raw error message, etc.
};

type Choice = {
  /**
   * The reason the model stopped generating tokens. This will be `stop` if the model
   * hit a natural stop point or a provided stop sequence, `length` if the maximum
   * number of tokens specified in the request was reached, `content_filter` if
   * content was omitted due to a flag from our content filters, `tool_calls` if the
   * model called a tool, or `function_call` (deprecated) if the model called a
   * function.
   */
  finish_reason:
    | "stop"
    | "length"
    | "tool_calls"
    | "content_filter"
    | "function_call";

  /**
   * The index of the choice in the list of choices.
   */
  index: number;

  /**
   * Log probability information for the choice.
   */
  logprobs: null;

  /**
   * A chat completion message generated by the model.
   */
  message: ChatCompletionMessage;

  /**
   * Malformed function call from Gemini models.
   * https://github.com/google-gemini/gemini-cli/issues/4486
   */
  error?: ErrorResponse;
};

/**
 * A chat completion message generated by the model.
 */
type ChatCompletionMessage = {
  /**
   * The contents of the message.
   */
  content: string | null;

  reasoning: string | null;

  /**
   * The refusal message generated by the model.
   */
  refusal: string | null;

  /**
   * The role of the author of this message.
   */
  role: "assistant";

  /**
   * If the audio output modality is requested, this object contains data about the
   * audio response from the model.
   * [Learn more](https://platform.openai.com/docs/guides/audio).
   */
  audio?: ChatCompletionAudio | null;

  /**
   * The tool calls generated by the model, such as function calls.
   */
  tool_calls?: ChatCompletionMessageToolCall[];
};

type ChatCompletionMessageToolCall = {
  /**
   * The ID of the tool call.
   */
  id: string;

  /**
   * The function that the model called.
   */
  function: {
    /**
     * The arguments to call the function with, as generated by the model in JSON
     * format. Note that the model does not always generate valid JSON, and may
     * hallucinate parameters not defined by your function schema. Validate the
     * arguments in your code before calling your function.
     */
    arguments: string;

    /**
     * The name of the function to call.
     */
    name: string;
  };

  /**
   * The type of the tool. Currently, only `function` is supported.
   */
  type: "function";
};

/**
 * If the audio output modality is requested, this object contains data about the
 * audio response from the model.
 * [Learn more](https://platform.openai.com/docs/guides/audio).
 */
type ChatCompletionAudio = {
  /**
   * Unique identifier for this audio response.
   */
  id: string;

  /**
   * Base64 encoded audio bytes generated by the model, in the format specified in
   * the request.
   */
  data: string;

  /**
   * The Unix timestamp (in seconds) for when this audio response will no longer be
   * accessible on the server for use in multi-turn conversations.
   */
  expires_at: number;

  /**
   * Transcript of the audio generated by the model.
   */
  transcript: string;
};

/**
 * https://github.com/openai/openai-node/blob/master/src/resources/completions.ts
 */
export type OpenAIChatCompletionResponse = {
  /**
   * A unique identifier for the chat completion.
   */
  id: string;

  /**
   * A list of chat completion choices. Can be more than one if `n` is greater
   * than 1.
   */
  choices: Choice[];

  /**
   * The Unix timestamp (in seconds) of when the chat completion was created.
   */
  created: number;

  /**
   * The model used for the chat completion.
   */
  model: string;

  /**
   * The object type, which is always `chat.completion`.
   */
  object: "chat.completion";

  /**
   * Usage statistics for the completion request.
   */
  usage?: CompletionUsage;
};

type AnthropicTextResponse = {
  type: "text";
  text: string;
};
type AnthropicToolUseResponse = {
  type: "tool_use";
  id: string;
  name: string;
  input: AnyObject;
};
export type AnthropicChatCompletionResponse = {
  id: string;
  type: "message";
  role: "assistant";
  /**
   * "claude-3-5-sonnet-20240620" |
   */
  model: string;
  content: (AnthropicTextResponse | AnthropicToolUseResponse)[];
  stop_reason: "tool_use";
  stop_sequence: null;
  usage: {
    input_tokens: number;
    cache_creation_input_tokens: number;
    cache_read_input_tokens: number;
    output_tokens: number;
  };
};

export type GoogleGeminiChatCompletionResponse = {
  candidates: {
    content: {
      parts: (
        | {
            text: string;
          }
        | {
            functionCall: {
              name: string;
              args: AnyObject | undefined;
            };
          }
      )[];
      role: "model";
    };
    finishReason: "STOP";
    avgLogprobs: number;
  }[];
  usageMetadata: {
    totalTokenCount: number;
    promptTokenCount: number;
    promptTokensDetails: [{ modality: "TEXT"; tokenCount: number }];
    candidatesTokenCount: number;
    candidatesTokensDetails: [{ modality: "TEXT"; tokenCount: number }];
  };
  modelVersion: "gemini-1.5-flash";
};
